# Distributed_Systems_4

**Source:** [[Distributed_Systems_4.pdf]]
**Date:** 2025-10-03
**Tags:** #technical-book #programming #notes

## Key Concepts

- The following is a list of key technical concepts and important points from this text: 1. The following is a list of key technical concepts and important points from this text: 2. The following is a list of key technical concepts and important points from this text: 3. The following is a list of key technical concepts and important points from this text: 4. The following is a list of key technical concepts and important points from this text: 5. The following is a list of key technical concepts and important points from this text: 6. The following is a list of key technical concepts and important points from this text: 7. The following is a list of key technical concepts and important points from this text: 8. The following is a list of key technical concepts and important points from this text: 9. The following is a list of key technical concepts and important points from this text: 10. The following is a list of key technical concepts and important points from this text: 11. The following is a list of key technical concepts and important points from this text: 12. The following is a list of key technical concepts and important points from this text: 13. The following is a list of key technical concepts and important points from this text: 14. The following is
- The first view is that there was a need to connect existing (networked) computer systems to each other. This may happen when services running on a system need to be made available to users and applica- tions that were not thought of before. In the scientific-research domain, we have seen efforts to connect a myriad of often expensive resources (special-purpose computers, supercomputers, very large database systems, etc.) into what came to be known as a grid computer. The second view is that an existing system required an expansion through additional computers. This view is the one most often related to the field of distributed systems. Before we discuss why this distinction is important, let us look at a few examples of each type of system. Decentralized systems are mainly related to the integrative view of net- worked computer systems. They come to being because we want to con- nect systems, yet may be hindered by administrative boundaries. For exam- ple, many applications in the artificial-intelligence domain require massive amounts of data for building reliable predictive models. The result is known as federated learning, DS 4.03x downloaded by kevin.sullivan@s
- 1.1. FROM NETWORKED SYSTEMS TO DISTRIBUTED SYSTEMS 9 Distributed systems are all about processes. The process perspective is all about understanding the different forms of processes that occur in distributed systems, be they threads, their virtualization of hardware processes, clients, servers, and so on. To make distributed systems work, what happens under the hood on top of which applications are executed, is that processes coordinate things. They jointly coordinate, for example, to compensate for the lack of global clock, for realizing mutual exclusive access to shared resources, and so on. To access processes and resources, we need naming. In particular, we need naming schemes that, when used, will lead to the process, resources, or whatever other type of entity that is being named. A critical aspect of distributed systems is that they perform well in terms of efficiency and in terms of dependability. The key instrument for both aspects is replicating resources. The only problem with replication is that updates may happen, implying that all copies of a resource need to be updated as well. The perspective of fault tolerance dives into the means for masking failures and their recovery. It has proven to be one of the
- 1.2. DESIGN GOALS 11 special shared folder that is maintained by a third party somewhere on the In- ternet. Using special software, the shared folder is barely distinguishable from other folders on a users computer. In effect, these services replace the use of a shared directory on a local distributed file system, making data available to users independent of the organization they belong to, and independent of where they are. 1.2.2 Distribution transparency An important goal of a distributed system is to hide the fact that its processes and resources are physically distributed across multiple computers, possibly separated by large distances. As we shall discuss more extensively in Chapter 2, achieving distribution transparency is realized through what is known as middleware, sketched in Figure 1.2 (see Gazis and Katsiri [2022] for a first introduction). In essence, what applications get to see is the same interface everywhere, whereas behind that interface, where and how processes and resources are and how they are accessed is kept transparent. We use the term object to mean either a process or a resource. Access transparency deals with hiding differences in data representation and the way that objects can be accessed. At a basic level, we
- Transparency Description Access Hide differences in data representation and how an object is accessed Location Hide where an object is located Relocation Hide that an object may be moved to another location while in use Migration Hide that an object may move to another location Replication Hide that an object may be shared by several independent users Failure Hide the failure and recovery of an object Figure 1.3: Different forms of transparency in a distributed system (see ISO [1995]). Location transparency refers to the fact that users cannot tell where an object is physically located in the system. Naming plays an important role in achieving location transparency. Location transparency can often be achieved by assigning only logical names to resources, that is, names in which the location of a resource is not secretly encoded. An example of a such a name is the uniform resource locator (URL) https://www.distributed-systems.net/, which gives no clue about the actual location of the Web server where this book is offered. We return to cloud computing in subsequent chapters, and, in particular, in Chapter 2. Where relocation transparency refers to being moved by the distributed system, migration transparency is offered by a distributed system
- 1.2. DESIGN GOALS 13 are actually moving, mobile phones will allow them to continue their con- versation. Other examples that come to mind include online tracking and tracing of goods as they are being transported from one place to another, and teleconferencing (partly) using devices that are equipped with mobile Internet. Replication plays an important role in distributed systems. Replication transparency deals with hiding the fact that several copies of a resource exist, or that several processes are operating in some form of lockstep mode so that one can take over when another fails. Competition transparency deals with concurrent access to a shared resource. Consistency can be achieved through locking mechanisms, by which users are, in turn, given exclusive access to the desired resource. Last, but certainly not least, it is important that a distributed system provides failure transparency. This means that a user or application does not notice that some piece of the system fails to work properly, and that the system subsequently (and automatically) recovers from that failure.
- 1.2. DESIGN GOALS 17 Separating policy from mechanism To achieve flexibility in open distributed systems, it is crucial that the system be organized as a collection of relatively small and easily replaceable or adaptable components. This implies that we should provide definitions of not only the highest-level interfaces, that is, those seen by users and applications, but also definitions for interfaces to internal parts of the system and describe how those parts interact. This approach is relatively new. Many older and even contemporary systems are constructed using a monolithic approach in which components are only logically separated but implemented as one, huge program. This approach makes it hard to replace or adapt a component without affecting the entire system. Caches are most effective when a browser can return pages without having to contact the original Website. Note also that refresh rates are highly dependent on which data is actually cached: whereas timetables for trains hardly change, this is not the case for Web pages showing current highway- traffic conditions, or worse yet, stock prices. What we need is a separation between policy and mechanism. In the case of Web caching, for example, a browser should ideally provide facilities for only storing documents (
- In theory, strictly separating policies from mechanisms seems to be the way to go. However, there is an important trade-off to consider: the stricter the separation, the more we need to make sure that we offer the appropriate collection of mechanisms. In practice, this means that a rich set of features is offered, in turn leading to many configuration parameters. One option to alleviate these problems is to provide reasonable defaults, and this is what often happens in practice. 1.2.4 Dependability As its name suggests, dependability in distributed systems can be rather intricate due to partial failures: somewhere there is a component failing while the system as a whole still seems to be living up to expectations (up to a certain point or moment). Although single-computer systems can also suffer from failures that do not appear immediately, having a potentially large collection of networked computer systems complicates matters considerably. In fact, one should assume that at any time, there are always partial failures occurring. An important goal of distributed systems is to mask those failures, as well as mask the recovery from those failures.
- 1.2. DESIGN GOALS 21 1.2.5 Security A distributed system that is not secure, is not dependable. As mentioned, special attention is needed to ensure confidentiality and integrity, both of which are directly coupled to authorized disclosure and access of information and resources. In any computer system, authorization is done by checking whether an identified entity has proper access rights. In turn, this means that the system should know it is indeed dealing with the proper entity. For this reason alone, proper authorization is important, as it may be used to limit any damage that a person, who could in hindsight not be trusted, can cause. For this reason alone, proper authorization is important, as it may be used to limit any damage that a person, who could in hindsight not be trusted, can cause.
- Encryption and decryption in public-key systems can be used in two, fundamentally different ways. First, if Alice wants to encrypt data that can be decrypted only by Bob, she should use Bob's public key, PKB, leading to the encrypted data PKB(data). Only the holder of the associated secret key can decrypt this information, i.e., Bob, who will apply the operation SKB(PKB(data)), which returns data. A second, and widely applied use case, is that of realizing digital signatures. Suppose Alice makes some data available for which it is important that any party, but let us assume it is Bob, needs to know for sure that it comes from Alice. In that case, Alice can encrypt the data with her secret key SKA, leading to SKA(data). If it can be assured that the associated public key PKA indeed belongs to Alice, then successfully decrypting SKA(data) to data, is proof that Alice knows about data: she is the only one holding the secret key SKA. Of course, we need to make the assumption that Alice is indeed the only one who holds

## Detailed Notes

### Page 17

• The following is a list of key technical concepts and important points from this text: 1. The following is a list of key technical concepts and important points from this text: 2. The following is a list of key technical concepts and important points from this text: 3. The following is a list of key technical concepts and important points from this text: 4. The following is a list of key technical concepts and important points from this text: 5. The following is a list of key technical concepts and important points from this text: 6. The following is a list of key technical concepts and important points from this text: 7. The following is a list of key technical concepts and important points from this text: 8. The following is a list of key technical concepts and important points from this text: 9. The following is a list of key technical concepts and important points from this text: 10. The following is a list of key technical concepts and important points from this text: 11. The following is a list of key technical concepts and important points from this text: 12. The following is a list of key technical concepts and important points from this text: 13. The following is a list of key technical concepts and important points from this text: 14. The following is

### Page 18

• The pace at which computer systems change was, is, and continues to be overwhelming. From 1945, when the modern computer era began, until about 1985, computers were large and expensive. Moreover, lacking a way to connect them, these computers operated independently of one another. Starting in the mid-1980s, two advances in technology began to change that situation. The first was the development of powerful microproces- sors. Initially, these were 8-bit machines, but soon 16-, 32-, and 64-bit CPUs became common. With powerful multicore CPUs, we now are again facing the challenge of adapting and developing programs to exploit parallelism. In any case, the current generation of machines have the computing power of the mainframes deployed 30 or 40 years ago, but for 1/1000th of the price or less. The second development was the invention of high-speed computer net- works. Local-area networks or LANs allow thousands of machines within a building to be connected in such a way that small amounts of information can be transferred in a few microseconds or so. Larger amounts of data can be moved between machines at rates of billions of bits per second

### Page 19

• 1.1. FROM NETWORKED SYSTEMS TO DISTRIBUTED SYSTEMS 3 The size of a networked computer system may vary from a handful of devices, to millions of computers. The interconnection network may be wired, wireless, or a combination of both. Moreover, these systems are often highly dynamic, in the sense that computers can join and leave, with the topology and performance of the underlying network almost continuously changing. It is difficult to think of computer systems that are not networked. And as a matter of fact, most networked computer systems can be accessed from any place in the world because they are hooked up to the Internet. Studying to understand these systems can easily become exceedingly complex. In this chapter, we start with shedding some light on what needs to be understood to build up the bigger picture without getting lost. 1.1 From networked systems to distributed systems Before we dive into various aspects of distributed systems, let us first consider what distribution, or decentralization, actually entails. 1.1.1 Distributed versus decentralized systems When considering various sources, there are quite a few opinions on dis- tributed versus decentralized

### Page 20

• The first view is that there was a need to connect existing (networked) computer systems to each other. This may happen when services running on a system need to be made available to users and applica- tions that were not thought of before. In the scientific-research domain, we have seen efforts to connect a myriad of often expensive resources (special-purpose computers, supercomputers, very large database systems, etc.) into what came to be known as a grid computer. The second view is that an existing system required an expansion through additional computers. This view is the one most often related to the field of distributed systems. Before we discuss why this distinction is important, let us look at a few examples of each type of system. Decentralized systems are mainly related to the integrative view of net- worked computer systems. They come to being because we want to con- nect systems, yet may be hindered by administrative boundaries. For exam- ple, many applications in the artificial-intelligence domain require massive amounts of data for building reliable predictive models. The result is known as federated learning, DS 4.03x downloaded by kevin.sullivan@s

### Page 21

• 1.1. From Networked Systems to Distributed Systems 5 and is implemented by a decentralized system, where the need for spreading processes and resources is dictated by administrative policies. Another example of a decentralized system is that of distributed ledger, also known as a blockchain. In this case, we need to deal with the situation that participating parties do not trust each other enough to set up simple schemes for collaboration. In this case, consider systems that are naturally geographically dispersed. As we mentioned, distributed systems are mainly related to the expansive view of networked computer systems. A well-known example is making use of e-mail services, such as Google Mail. What often happens is that a user logs into the system through a Web interface to read and send mails. More often, however, is that users configure their personal computer (such as a laptop) to make use of a specific mail client. To that end, they need to configure a few settings, such as the incoming and outgoing server. In the case of Google Mail, these are imap.gmail.com and smtp.gmail.com, respectively. Logically, it seems as

### Page 22

• A logically centralized solution can be implemented in a highly scalable distributed manner. A logically centralized solution can be implemented in a highly scalable distributed manner.

### Page 23

• 1.1. From Networked Systems to Distributed Systems 7 organized as a huge tree, where each path from the root to a leaf node represents a fully qualified name, such as www.distributed-systems.net. It would be a mistake to think that the root node is implemented by just a single server. In fact, as we shall encounter many times throughout this book, (logically, and even physically) centralized solutions are often much better than distributed counterparts for the simple reason that there is a single point of failure. It makes them much easier to manage, for example, and certainly in comparison where there may be multiple points of failures. Moreover, that single point of failure can be hardened against many kinds of failures as well as many kinds of security attacks. When it comes to being a performance bottleneck, we will also see that many things can be done to ensure that even that cannot be held against centralization. In this sense, let us not forget that centralized solutions have even proven to be extremely scalable and robust. They are called cloud-based solutions. Again, their implementations can make use of very sophisticated distributed solutions, yet even then, we

### Page 24

• DS 4.03x Downloaded by kevin.sullivan@sullivansoftware.dev

### Page 25

• 1.1. FROM NETWORKED SYSTEMS TO DISTRIBUTED SYSTEMS 9 Distributed systems are all about processes. The process perspective is all about understanding the different forms of processes that occur in distributed systems, be they threads, their virtualization of hardware processes, clients, servers, and so on. To make distributed systems work, what happens under the hood on top of which applications are executed, is that processes coordinate things. They jointly coordinate, for example, to compensate for the lack of global clock, for realizing mutual exclusive access to shared resources, and so on. To access processes and resources, we need naming. In particular, we need naming schemes that, when used, will lead to the process, resources, or whatever other type of entity that is being named. A critical aspect of distributed systems is that they perform well in terms of efficiency and in terms of dependability. The key instrument for both aspects is replicating resources. The only problem with replication is that updates may happen, implying that all copies of a resource need to be updated as well. The perspective of fault tolerance dives into the means for masking failures and their recovery. It has proven to be one of the

### Page 26

• A distributed system should make resources easily accessible; it should hide the fact that resources are distributed across a network; it should be open; and it should be scalable. A distributed system should make resources easily accessible; it should hide the fact that resources are distributed across a network; it should be open; and it should be scalable. A distributed system should make resources easily accessible; it should hide the fact that resources are distributed across a network; it should be open; and it should be scalable. A distributed system should make resources easily accessible; it should hide the fact that resources are distributed across a network; it should be open; and it should be scalable. A distributed system should make resources easily accessible; it should hide the fact that resources are distributed across a network; it should be open; and it should be scalable. A distributed system should make resources easily accessible; it should hide the fact that resources are distributed across a network; it should be open; and it should be scalable. A distributed system should make resources easily accessible; it should hide the fact that resources are distributed across a network; it should be open; and it should be scalable. A distributed system should make resources easily accessible; it should

### Page 27

• 1.2. DESIGN GOALS 11 special shared folder that is maintained by a third party somewhere on the In- ternet. Using special software, the shared folder is barely distinguishable from other folders on a users computer. In effect, these services replace the use of a shared directory on a local distributed file system, making data available to users independent of the organization they belong to, and independent of where they are. 1.2.2 Distribution transparency An important goal of a distributed system is to hide the fact that its processes and resources are physically distributed across multiple computers, possibly separated by large distances. As we shall discuss more extensively in Chapter 2, achieving distribution transparency is realized through what is known as middleware, sketched in Figure 1.2 (see Gazis and Katsiri [2022] for a first introduction). In essence, what applications get to see is the same interface everywhere, whereas behind that interface, where and how processes and resources are and how they are accessed is kept transparent. We use the term object to mean either a process or a resource. Access transparency deals with hiding differences in data representation and the way that objects can be accessed. At a basic level, we

### Page 28

• Transparency Description Access Hide differences in data representation and how an object is accessed Location Hide where an object is located Relocation Hide that an object may be moved to another location while in use Migration Hide that an object may move to another location Replication Hide that an object may be shared by several independent users Failure Hide the failure and recovery of an object Figure 1.3: Different forms of transparency in a distributed system (see ISO [1995]). Location transparency refers to the fact that users cannot tell where an object is physically located in the system. Naming plays an important role in achieving location transparency. Location transparency can often be achieved by assigning only logical names to resources, that is, names in which the location of a resource is not secretly encoded. An example of a such a name is the uniform resource locator (URL) https://www.distributed-systems.net/, which gives no clue about the actual location of the Web server where this book is offered. We return to cloud computing in subsequent chapters, and, in particular, in Chapter 2. Where relocation transparency refers to being moved by the distributed system, migration transparency is offered by a distributed system

### Page 29

• 1.2. DESIGN GOALS 13 are actually moving, mobile phones will allow them to continue their con- versation. Other examples that come to mind include online tracking and tracing of goods as they are being transported from one place to another, and teleconferencing (partly) using devices that are equipped with mobile Internet. Replication plays an important role in distributed systems. Replication transparency deals with hiding the fact that several copies of a resource exist, or that several processes are operating in some form of lockstep mode so that one can take over when another fails. Competition transparency deals with concurrent access to a shared resource. Consistency can be achieved through locking mechanisms, by which users are, in turn, given exclusive access to the desired resource. Last, but certainly not least, it is important that a distributed system provides failure transparency. This means that a user or application does not notice that some piece of the system fails to work properly, and that the system subsequently (and automatically) recovers from that failure.

### Page 30

• DS 4.03x Downloaded by kevin.sullivan@sullivansoftware.dev DS 4.03x may be a nice goal when designing and implementing distributed systems, but that DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev Notes: DS 4.03x may be a nice goal when designing and implementing distributed systems, but that DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev Notes: DS 4.03x may be a nice goal when designing and implementing distributed systems, but that DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev Notes: DS 4.03x may be a nice goal when designing and implementing distributed systems, but that DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev Notes: DS 4.03x may be a nice goal when designing and implementing distributed systems, but that DS 4.03x downloaded by kevin.sullivan@sullivansoftware.

### Page 31

**Note 1.2. DESIGN GOALS 15 it should be considered together with other issues such as performance and comprehensibility. Note 1.2 (Discussion: Against distribution transparency) Several researchers have argued that hiding distribution will lead to only further complicating the development of distributed systems, exactly for the reason that full distribution transparency can never be achieved. Note 1.2 (Discussion: Against distribution transparency) Several researchers have argued that hiding distribution will lead to only further complicating the development of distributed systems, exactly for the reason that full distribution transparency can never be achieved. Note 1.2 (Discussion: Against distribution transparency) Several researchers have argued that hiding distribution will lead to only further complicating the development of distributed systems, exactly for the reason that full distribution transparency can never be achieved. Note 1.2 (Discussion: Against distribution transparency) Several researchers have argued that hiding distribution will lead to only further complicating the development of distributed systems, exactly for the reason that full distribution transparency can never be achieved. Note 1.2 (Discussion: Against distribution transparency) Several researchers have argued that hiding distribution will lead to only further complicating the development of distributed systems, exactly for**

### Page 32

• DS 4.03x Downloaded by kevin.sullivan@sullivansoftware.dev Note 1.3 (Discussion: Open systems in practice)

### Page 33

• 1.2. DESIGN GOALS 17 Separating policy from mechanism To achieve flexibility in open distributed systems, it is crucial that the system be organized as a collection of relatively small and easily replaceable or adaptable components. This implies that we should provide definitions of not only the highest-level interfaces, that is, those seen by users and applications, but also definitions for interfaces to internal parts of the system and describe how those parts interact. This approach is relatively new. Many older and even contemporary systems are constructed using a monolithic approach in which components are only logically separated but implemented as one, huge program. This approach makes it hard to replace or adapt a component without affecting the entire system. Caches are most effective when a browser can return pages without having to contact the original Website. Note also that refresh rates are highly dependent on which data is actually cached: whereas timetables for trains hardly change, this is not the case for Web pages showing current highway- traffic conditions, or worse yet, stock prices. What we need is a separation between policy and mechanism. In the case of Web caching, for example, a browser should ideally provide facilities for only storing documents (

### Page 34

• In theory, strictly separating policies from mechanisms seems to be the way to go. However, there is an important trade-off to consider: the stricter the separation, the more we need to make sure that we offer the appropriate collection of mechanisms. In practice, this means that a rich set of features is offered, in turn leading to many configuration parameters. One option to alleviate these problems is to provide reasonable defaults, and this is what often happens in practice. 1.2.4 Dependability As its name suggests, dependability in distributed systems can be rather intricate due to partial failures: somewhere there is a component failing while the system as a whole still seems to be living up to expectations (up to a certain point or moment). Although single-computer systems can also suffer from failures that do not appear immediately, having a potentially large collection of networked computer systems complicates matters considerably. In fact, one should assume that at any time, there are always partial failures occurring. An important goal of distributed systems is to mask those failures, as well as mask the recovery from those failures.

### Page 35

• 1.2. DESIGN GOALS 19 Availability is defined as the probability that the system is operating correctly at any given moment and is available to perform its functions on behalf of its users. Reliability is defined as the property that a system can run continuously without failure. Safety is defined as the situation that when a system temporarily fails to operate correctly, no catastrophic event happens. Maintainability is defined as how easily a failed system can be repaired. Traditionally, fault-tolerance has been related to the following three metrics: Mean Time To Failure (MTTF), Mean Time To Repair (MTTR), Mean Time Between Failures (MTBF), Mean Time Between Failures (MTBF), Mean Time Between Failures (MTBF), Mean Time Between Failures (MTBF), Mean Time Between Failures (MTBF), Mean Time Between Failures (MTBF), Mean Time Between Failures (MTBF), Mean Time Between Failures (MTBF), Mean Time Between Failures (MTBF), Mean Time Between Failures (MTBF), Mean Time Between Failures (MTBF), Mean Time Between Failures (MTBF), Mean Time Between Failures (MTBF), Mean Time Between Failures (MTBF), Mean Time Between

### Page 36

• DS 4.03x Downloaded by kevin.sullivan@sullivansoftware.dev

### Page 37

• 1.2. DESIGN GOALS 21 1.2.5 Security A distributed system that is not secure, is not dependable. As mentioned, special attention is needed to ensure confidentiality and integrity, both of which are directly coupled to authorized disclosure and access of information and resources. In any computer system, authorization is done by checking whether an identified entity has proper access rights. In turn, this means that the system should know it is indeed dealing with the proper entity. For this reason alone, proper authorization is important, as it may be used to limit any damage that a person, who could in hindsight not be trusted, can cause. For this reason alone, proper authorization is important, as it may be used to limit any damage that a person, who could in hindsight not be trusted, can cause.

### Page 38

• Encryption and decryption in public-key systems can be used in two, fundamentally different ways. First, if Alice wants to encrypt data that can be decrypted only by Bob, she should use Bob's public key, PKB, leading to the encrypted data PKB(data). Only the holder of the associated secret key can decrypt this information, i.e., Bob, who will apply the operation SKB(PKB(data)), which returns data. A second, and widely applied use case, is that of realizing digital signatures. Suppose Alice makes some data available for which it is important that any party, but let us assume it is Bob, needs to know for sure that it comes from Alice. In that case, Alice can encrypt the data with her secret key SKA, leading to SKA(data). If it can be assured that the associated public key PKA indeed belongs to Alice, then successfully decrypting SKA(data) to data, is proof that Alice knows about data: she is the only one holding the secret key SKA. Of course, we need to make the assumption that Alice is indeed the only one who holds

### Page 39

• 1.2. DESIGN GOALS 23 Bi (for example, as the result of an attack), will require that the attacker also changes the stored hash value in Bi+1. However, changing that value, also means changing the hash value of Bi+1, and thus the value stored in Bi+2, in turn, requiring that a new hash value is to be computed for Bi+2, and so on. Cryptography is also used for another important mechanism in distributed systems: delegating access rights. Delegation is something we are now used to: many of us delegate access rights that we have as a user to specific applications, such as an e-mail client. An upcoming distributed application of cryptography is so-called multi-party computation: the means for two or three parties to compute a value for which the data of those parties is needed, but without having to actually share that data.

### Page 40

• Scalability dimensions Scalability of a system can be measured along at least three different dimensions (see [Neuman, 1994]): Size scalability: A system can easily add more users and resources to the system without any noticeable loss of performance. Geographical scalability: A geographically scalable system is one in which the users and resources may lie far apart, but the fact that communication delays may be significant is hardly noticed. Administrative scalability: An administratively scalable system can still be easily managed even if it spans many independent adminis- trative organizations such as Googles Chromebook.

### Page 41

• 1.2. DESIGN GOALS 25 The network between the user and the centralized service Let us first consider the computational capacity. Just imagine a service for computing optimal routes taking real-time traffic information into account. If there is only a single machine available, then even a modern high-end system will eventually run into problems if the number of requests exceeds a certain point. Finally, the network between the user and the service may also be the cause of poor scalability. Just imagine a video-on-demand service that needs to stream high-quality video to multiple users. A video stream can easily require a bandwidth of 8 to 10 Mbps, meaning that if a service sets up point-to-point connections with its customers, it may soon hit the limits of the network capacity of its own outgoing transmission lines. There are several solutions to attack size scalability, which we discuss below after having looked into geographical and administrative scalability. Note 1.5 (Advanced: Analyzing size scalability) Figure 1.5: A simple model of a service as a queuing system.

### Page 42

• DS 4.03x Downloaded by kevin.sullivan@sullivansoftware.dev

### Page 43

• 1.2. DESIGN GOALS 27 that were designed for local-area networks is that many of them are based on synchronous communication. In this form of communication, a party requesting a service, generally referred to as a client, blocks until a reply is sent back from the server implementing the service. More specifically, we often see a communication pattern consisting of many client-server interactions, as may be the case with database transactions. Another problem that hinders geographical scalability is that communication in wide-area networks is inherently much less reliable than in local-area networks. In addition, we generally also need to deal with limited bandwidth. Yet another issue that pops up when components lie far apart is the fact that wide-area systems generally have only very limited facilities for multipoint communication. In contrast, local-area networks often support efficient broadcasting mechanisms. Such mechanisms have proven to be extremely useful for discovering components and services, which is essential from a management perspective. In wide-area systems, we need to develop separate services, such as naming and directory services, to which queries can be sent. These support services, in turn, need to be scalable as well and often no obvious solutions

### Page 44

• DS 4.03x Downloaded by kevin.sullivan@sullivansoftware.dev

### Page 45

• 1.2. DESIGN GOALS 29 M A A R T E N FIRST NAME LAST NAME E-MAIL Server Client Check form Process form MAARTEN MVS VAN-STEEN.NET @ VAN STEEN (a) FIRST NAME LAST NAME E-MAIL Server Client Check form Process form MAARTEN MVS@VAN-STEEN.NET VAN STEEN MAARTEN VAN STEEN MAARTEN VAN STEEN MVS@VAN-STEEN.NET (b) Figure 1.6: The difference between letting (a) a server or (b) a client check forms as they are being filled. Hiding communication latencies Hiding communication latencies is appli- cable in the case of geographical scalability. The basic idea is simple: try to avoid waiting for responses to remote-service requests as much as possible. For example, when a service has been requested at a remote machine, an alternative to waiting for a reply from the server is to do other useful work at the requesters side. Essentially, this means constructing the requesting appli- cation in such a way that it uses only

### Page 46

• DS 4.03x Downloaded by kevin.sullivan@sullivansoftware.dev Conceptually, DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev

### Page 47

• 1.2. DESIGN GOALS 31 it may even appear as if there is only a single server. However, the Web is physically partitioned and distributed across a few hundreds of millions of servers, each handling often a number of Websites, or parts of Websites. The name of the server handling a document is encoded into that documents URL. It is only because of this distribution of documents that the Web has been capable of scaling to its current size. Yet, note that finding out how many servers provide Web-based services is virtually impossible: A Website today is so much more than a few static Web documents. Replication Considering that scalability problems often appear in the form of performance degradation, it is generally a good idea to actually replicate components or resources, etc., across a distributed system. Replication not only increases availability, but also helps to balance the load between com- ponents, leading to better performance. Moreover, in geographically widely dispersed systems, having a copy nearby can hide much of the communication latency problems mentioned earlier. Caching is a special form of replication, although the distinction between the two is often hard to make or even artificial. There

### Page 48

• DS 4.03x Downloaded by kevin.sullivan@sullivansoftware.dev

### Page 49

• 1.3. A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS 33 Note 1.6 (More information: Parallel processing) High-performance computing more or less started with the introduction of multi- processor machines. In this case, multiple CPUs are organized in such a way that they all have access to the same physical memory, as shown in Figure 1.8(a). However, in a multicomputer system several computers are connected through a network and there is no sharing of main memory, as shown in Figure 1.8(b). To overcome the limitations of shared-memory systems, high-performance computing moved to distributed-memory systems. This shift also meant that many programs had to make use of message passing instead of modifying shared data as a means of communication and synchronization between threads. Unfortunately, message-passing models have proven to be much more difficult and error-prone compared to the shared-memory programming models. For this reason, there has been significant research in attempting to build so-called distributed shared- memory multicomputers, or simply DSM systems [Amza et al., 1996].

### Page 50

• Cluster computing Cluster computing systems became popular when the price/performance ratio of personal computers and workstations improved. At a certain point, it became financially and technically attractive to build a supercomputer using off-the-shelf technology by simply hooking up a collection of relatively simple computers in a high-speed network. In virtually all cases, cluster computing is used for parallel programming, in which a single (compute intensive) program is run in parallel on multiple machines. The principle of this organization is shown in Figure 1.9. Cluster computing has evolved considerably. As discussed extensively by Gerofi et al. [2019], the developments of supercom- puters organized as clusters have reached a point where we see clusters with more than 100,000 CPUs, with each CPU having 8 or 16 cores. There are mul- tiple networks. Most important is a network formed by dedicated high-speed interconnects between the various nodes (in other words, there is often no such thing as a shared high-speed network for computations).

### Page 51

• 1.3. A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS 35 the organization and performance of the system as a whole. In addition, a special high-performance file system or database is used, again with its own local, dedicated network. Figure 1.9 does not show additional equipment, notably high-speed I/O as well as networking facilities for remote access and communication. A management node is generally responsible for collecting jobs from users, to subsequently distribute the associated tasks among the various compute nodes. In practice, several management nodes are used when dealing with very large clusters. As such, a management node actually runs the software needed for the execution of programs and management of the cluster, while the compute nodes are equipped with a standard operating system extended with typical functions for communication, storage, fault tolerance, and so on. An interesting development, as explained in Gerofi et al. [2019], is the role of the operating system. There has been a clear trend to minimize the operating system to lightweight kernels, essentially ensuring the least possible overhead. A drawback is that such operating systems become highly spe- cialized and fine-tune

### Page 52

• The DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev The DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev

### Page 53

• 1.3. A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS 37 resource layer is thus seen to be responsible for access control, and hence will rely on the authentication performed as part of the connectivity layer. The next layer in the hierarchy is the collective layer. It deals with handling access to multiple resources and typically consists of services for resource discovery, allocation and scheduling of tasks onto multiple resources, data replication, and so on. Unlike the connectivity and resource layer, each consisting of a relatively small, standard collection of protocols, the collective layer may consist of many protocols reflecting the broad spectrum of services it may offer to a virtual organization and which make use of the grid computing environment. Typically, the collective, connectivity, and resource layer form the heart of what could be called a grid middleware layer. These layers jointly provide access to and management of resources that are potentially dispersed across multiple sites. An important observation from a middleware perspective is that in grid computing, the notion of a site (or administrative unit) is common. This prevalence is emphasized by the gradual shift toward a service-oriented ar- chitecture in which sites offer access to the various layers through

### Page 54

• Transactions require special primitives that must either be supplied by the underlying distributed system or by the language runtime system. Transactions require ordinary statements, procedure calls, and so on. The characteristic feature of a transaction is either all of these operations are executed or none are executed. These may be system calls, library procedures, or bracketing statements in a language, depending on the implementation. Transactions adhere to the so-called ACID properties: Atomic: To the outside world, the transaction happens indivisibly Consistent: The transaction does not violate system invariants DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev

### Page 55

• 1.3. A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS 39 Isolated: Concurrent transactions do not interfere with each other Durable: Once a transaction commits, the changes are permanent In distributed systems, transactions are often constructed as a number of subtransactions, jointly forming a nested transaction as shown in Figure 1.12. The top-level transaction may fork off children that run in parallel with one another, on different machines, to gain performance or simplify programming. Each of these children may also execute one or more subtransactions, or fork off its own children. Thus, the permanence referred to above applies only to top-level transactions. Since transactions can be nested arbitrarily deep, considerable administra- tion is needed to get everything right. The semantics are clear, however. When any transaction or subtransaction starts, it is conceptually given a private copy of all data in the entire system for it to manipulate as it wishes. If it aborts, its private universe just vanishes, as if it had never existed. If it commits, its private universe replaces the parents universe. Thus

### Page 56

• DS 4.03x Downloaded by kevin.sullivan@sullivansoftware.dev

### Page 57

• 1.3. A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS 41 Figure 1.14: Middleware as a communication facilitator in enterprise applica-tion integration. Several types of communication middleware exist. With remote procedure calls (RPC), an application component can effectively send a request to another application component by doing a local procedure call, which results in the request being packaged as a message and sent to the callee. As the popularity of object technology increased, techniques were devel- oped to allow calls to remote objects, leading to what is known as remote method invocations (RMI). An RMI is essentially the same as an RPC, except that it operates on objects instead of functions. RPC and RMI have the disadvantage that the caller and callee both need to be up and running at the time of communication. In addition, they need to know exactly how to refer to each other. This tight coupling is often experienced as a serious drawback, and has led to what is known as message-oriented middleware, or simply MOM. In this case, applications send messages to logical contact points, often described by a subject

### Page 58

• DS 4.03x Downloaded by kevin.sullivan@sullivansoftware.dev

### Page 59

• 1.3. A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS 43 to guarantee. In such cases, offering a messaging system carrying requests from the application A to perform an action at the application B, is what is needed. In such cases, messaging is not the panacea for application integration: it also introduces problems concerning data formatting and layout, it requires an application to know where to send a message to, there need to be scenarios for dealing with lost messages, and so on. Like RPCs, we will be discussing these issues extensively in Chapter 4. What these four approaches tell us, is that application integration will generally not be simple. Middleware (in the form of a distributed system), however, can significantly help in integration by providing the right facilities such as support for RPCs or messaging. 1.3.3 Pervasive systems The distributed systems discussed so far are largely characterized by their stability: nodes are fixed and have a more or less permanent and high-quality connection to a network. To a certain extent, this stability is realized through the various techniques for achieving distribution transparency. For example, there are many ways how we can create the illusion that only occasionally components may fail

### Page 60

• In a ubiquitous computing system, there will be devices close to users (such as sensors and actuators), connected to computers hidden from view, and perhaps even operating remotely in a DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev

### Page 61

• 1.3. A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS 45 cloud. Most, if not all, of the requirements regarding distribution transparency mentioned in Section 1.2.2, should therefore hold. Ad. 2: Interaction When it comes to interaction with users, ubiquitous computing systems differ a lot in comparison to the systems we have been discussing so far. For ubiquitous computing systems, much of the interaction by humans will be implicit, with an implicit action being defined as one that is not primarily aimed to interact with a com- puterized system but which such a system understands as input [Schmidt, 2000]. In other words, a user could be mostly unaware of the fact that input is being provided to a computer system. For ubiquitous computing systems, much of the interaction by humans will be implicit, with an implicit action being defined as one that is not primarily aimed to interact with a com- puterized system but which such a system understands as input [Schmidt, 2000]. In other words, a user could be mostly unaware of the fact that input is being provided to a computer system. For ubiquitous computing systems, much of the

### Page 62

• A review of the book.

### Page 63

• A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS A SIMPLE

### Page 64

• DS 4.03x Downloaded by kevin.sullivan@sullivansoftware.dev

### Page 65

• 1.3. A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS 49 Sensor networks Our last example of pervasive systems is sensor networks. These networks in many cases form part of the enabling technology for pervasiveness, and we see that many solutions for sensor networks return in pervasive applica- tions. What makes sensor networks interesting from a distributed systems perspective is that they are more than just a collection of input devices. In- stead, as we shall see, sensor nodes often collaborate to process the sensed data efficiently in an application- specific manner, making them very different from, for example, traditional computer networks. Many sensor networks use wireless communication, and the nodes are often battery powered. Their limited resources, restricted communication capabilities, and constrained power consumption demand that efficiency is high on the list of design criteria. When zooming into an individual node, we see that conceptually, they do not differ a lot from normal computers: above the hardware there is a software layer akin to what traditional operating systems offer, including low- level network access, access to sensors and actuators, memory management, and so on. However, similar to other network

### Page 66

• DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev Notes: DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev Notes: DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev Notes: DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev Notes: DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev Notes: DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev Notes: DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev Notes: DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev Notes: DS 4.03x downloaded by kevin.sullivan@sullivansoftware.dev Notes: DS 4.03x downloaded by kevin.sullivan@sullivansoftware.

### Page 67

• 1.3. A SIMPLE CLASSIFICATION OF DISTRIBUTED SYSTEMS 51 (a) (b) Figure 1.16: Organizing a sensor network database, while storing and processing data (a) only at the operators site or (b) only at the sensors. What happens when network links fail? These questions have been partly addressed in TinyDB, which implements a declarative (database) interface to wireless sensor networks [Madden et al., 2005]. In essence, TinyDB can use any tree-based routing algorithm. An intermediate node will collect and aggregate the results from its children, along with its own findings, and send that toward the root. To make matters efficient, queries span a period of time, allowing for careful scheduling of operations so that network resources and energy are optimally consumed. However, when queries can be initiated from different points in the net- work, using single-rooted trees such as in TinyDB may not be efficient enough enough. Sensor networks may be equipped with special nodes where results are forwarded to, as well as the queries related to those results. This approach corresponds directly to the notion of publish-subscribe

### Page 68

**Figure 1.17:** A hierarchical view from clouds to devices (adapted from Yousef- pour et al. [2019]) actuator is one that controls the temperature in a room, or switches devices on or off. By viewing and organizing the network as a distributed system, an operator is provided with a higher level of abstraction to monitor and control a situation. Cloud, edge, things As may have become clear by now, distributed systems span a huge range of different networked computer systems. Many of such systems operate in a setting in which the various computers are connected through a local-area network. Yet with the growth of the Internet-of-Things and the connectivity with remote services offered through cloud-based systems, new organizations across wide-area networks are emerging. Figure 1.17 presents this more hierarchical approach. Typically, higher up the hierarchy we see that typical qualities of dis- tributed systems improve: they become more reliable, have more capacity, and, in general, perform better. Lower in the hierarchy, we see that location- related aspects are easier facilitated, as well as performance qualities related to latencies. At the same time, the lower parts show in an

### Page 69

• A distributed system is a collection of networked computer systems in which processes and resources are spread across different computers. We make a distinction between sufficiently and necessarily spread, where the latter relates to decentralized systems.

### Page 70

• DS 4.03x Downloaded by kevin.sullivan@sullivansoftware.dev
